---
layout: single
title: Writing and its Double / L'écriture et son double
date: 2019-05-06 00:35:19.240800200 +0100
permalink: /writing-double/
languages: 
- English
- français
pitch:
- "Peritexts / Péritextes"
---

<blockquote class='FR'>
  <p>L'écriture à l'épreuve de la machine. Péritextes présentés à l'occasion de l'exposition <a href="/ca-artificiel/"><nobr style="font-style: normal">Ça artificiel</nobr></a> à Lausanne</p>
</blockquote>

## Aube neuronale

Le réseau de neurones occupe une position médiane, inconfortable, et encore très largement impensée, dans le champ des sciences contemporaines. Poussée technologique récente, malgré des bases théoriques remontant aux années d'après-guerre, elle dépasse en souplesse et possibilités les «&nbsp;outils&nbsp;» informatiques précédents. Certains vont jusqu'à y voir la promesse de la «&nbsp;singularité&nbsp;», ou «&nbsp;Intelligence Artificielle complète&nbsp;», qui serait l'émergence à plus ou moins long terme d'une *conscience* artificielle, nous forçant à repenser de fond en comble notre place dans l'univers. Toutefois, pour qui en étudie les détails, cette machinerie peut tout aussi bien apparaître comme une forme élaborée de calcul probabiliste, ne dépassant que *quantitativement* les prouesses, que peu aujourd'hui qualifieraient à proprement parler *d'intelligentes*, des calculatrices ou programmes que nous sommes habitués à manier. Je sens en moi ces deux positions s'affronter. D'un côté, une sorte de banalisation&nbsp;: l'innovation est là, mais elle est limitée, et bientôt ces prouessent seront aussi évidentes, intégrées au quotidien, que ce que nous avons aujourd'hui. D'un autre, je peux aussi soutenir le contraire, que le progrès est bien plus grand, et qu'il y a eu saut qualitatif. On peut alors, dans un mouvement rétroactif, rapprocher une grande partie de nos fonctionnements cognitifs dudit calcul, dans un geste qui s'apparenterait à de célèbres déchéances passées. La Terre n'est pas le centre de l'Univers, l'Homme n'est pas l'image de Dieu, le Moi n'est qu'une petite partie de nous-même, la Matière au sens commun n'est plus que la minorité de *ce qu'il y a* dans notre Univers peuplé «&nbsp;d'Énergie sombre&nbsp;» et «&nbsp;Matière noire&nbsp;»... Finalement, et ce serait une étape raisonnable, la plus grande partie de la conscience humaine, ses émotions, ses pensées, serait conceptualisable comme un travail d'approximation fonctionnelle et de prédictions probabilistes.

Que cette position se déploie comme une force majeure de notre temps n'est pas douteux. Toutefois, il n'est pas interdit de supposer qu'à l'opposé d'un aplanissement complet de «&nbsp;l'humain&nbsp;», au sens encore couramment donné à ce terme dans les domaines de la pensée, de l'émotion, du choix, de la création, on assiste à un *déplacement* de ces termes, et une redéfinition, sous la poussée de cette nouvelle présence, de l'architecture même de notre humanité. D'ici à dix ou vingt ans, on pourrait avoir, par exemple, de nouvelles distinctions&nbsp;: stupide, ou banal, serait ce qu'un réseau de neurones peut faire&nbsp;; intelligent ce qui échappe à leur portée (ainsi un «&nbsp;singe savant&nbsp;» roi du calcul mental, rivalisant tant bien que mal avec la puissance informatique disponible en 1960, n'est d'à peu près aucun intérêt pour la communauté scientifique, dont le processus de découverte intrinsèque requiert bien d'autres facultés).

Par ailleurs, et dans un mouvement parallèle, il est plus que probable que les dites «&nbsp;intelligences artificielles&nbsp;» seront aussi familières à ce moment-là que l'omniprésence des ordinateurs personnels le sont aujourd'hui -- nouveauté, rappelons-le, presque inconcevable en 1970 encore, et que certains jugent presque dépassés aujourd'hui, sous la poussée des smartphones et tablettes. On peut voir l'écriture «&nbsp;augmentée&nbsp;» (utilisant des réseaux de neurones ou autres techniques informatiques), selon une thèse sévère, comme pas grand chose de plus que le passage du manuscrit à Microsoft Word (chose encore révolutionnaire, soit dit en passant, pour certains professeurs de la Faculté des Lettres de cette université lorsque j'y étudiais il y a une décennie). Malgré son attrait, et même si ce serait la provocation idéale dans la saturation actuelle de verbiage mystico-scientiste entourant ces développements technologiques, je n'embrasse pas cette thèse, qui me semble méconnaître où se situe le travail, la collaboration, avec les réseaux de neurones, surtout dans un contexte artistique.

Le parallèle le plus flagrant, et celui qui offre à la fois le plus d'inspiration et de matériau pour le travail présent, est celui des «&nbsp;contraintes&nbsp;», ou «&nbsp;techniques formelles&nbsp;», que l'on trouve dans les arts d'avant-garde du siècle passé&nbsp;: en littérature, depuis le modernisme de Joyce jusqu'à l'Oulipo&nbsp;; en musique, avec la Seconde École de Vienne et les expérimentations d'après-guerre à Darmstadt, Paris et ailleurs&nbsp;; dans les arts plastiques, durant ces mêmes années d'explorations sérielles et abstraites.

## Une étape intermédiaire&nbsp;: l'ordinateur comme producteur total des possibles

L'ordinateur est l'expansion de l'outil même, qui, de son inertie et de sa passivité premières, devient ductile, surprenant, au point même où se pose, philosophiquement, la question de savoir si et quand on peut parler de puissance d'agir propre, de choix et libre arbitre, de la part de ces nouveaux systèmes algorithmiques. Jusqu'à présent, dans mon expérience propre, il s'est agi moins d'une compétition avec les capacités humaines qu'une nouvelles forme de collaboration, une complémentarité des compétences. L'ordinateur, dans ce nouveau champ des arts appelé «&nbsp;génératifs&nbsp;» ou «&nbsp;procédural&nbsp;», peut par exemple rendre actuel, produire, un ensemble de possibilités, un «&nbsp;champ&nbsp;», qui serait bien trop grand à réaliser «&nbsp;à la main&nbsp;» (sans parler de l'absurdité d'une telle tâche). Cette puissance d'exécution permet, lorsqu'on s'intéresse aux pratiques littéraires contraintes, de passer d'un mode de travail centré sur la difficulté de réalisation à un autre, centré sur ce qu'on pourrait appeler la recherche, ou l'enquête. Les oulipiens ont exploré avec obstination le premier champ, qui était par ailleurs la voie naturelle en un temps où l'ordinateur était encore l'apanage de l'armée et des laboratoires d'état. Prenez le patient travail d'un Perec composant *La Disparition* (1969), cherchant jour et nuit des mots sans la lettre «&nbsp;e&nbsp;», ou ses *Alphabets* (1976), poèmes de onze vers contenant à chaque vers les dix lettres les plus fréquentes de la langue française plus une lettre supplémentaire, qui donne la «&nbsp;tonalité&nbsp;» du texte -- un poème «&nbsp;en G&nbsp;», «&nbsp;en W&nbsp;» -- entièrement composés avec dictionnaire, papier, crayon, les listes de mots composés de certaines lettres devant être fastidieusement établie par l'écrivain.

Aujourd'hui il est possible d'établir une contrainte préalable, avec la «&nbsp;recette&nbsp;» correspondante (un algorithme) composée d'étapes claires de construction définissant un «&nbsp;champ de possibilités&nbsp;», et d'en laisser l'exécution à l'ordinateur. On peut être surpris de voir l'ampleur des résultats&nbsp;: lorsque je travaillais sur des carrés poétiques de lettres (ou «&nbsp;carrés magiques&nbsp;», où chaque ligne et colonne est un mot), je me retrouvais devant une tâche qui aurait été terriblement ardue en tant qu'humain, mais qui était de l'ordre du quotidien pour ma machine, qui produit sans broncher des *millions* de solutions possibles à partir d'un dictionnaire donné. La tâche, devant cette étendue surprenante (qui aurait dit, au vu de la difficulté pour moi d'en produire un seul, qu'il en existait *autant*?), est ensuite de *trouver* les textes intéressants, forts, *beaux*, dans cet insondable magma. Car, magma, c'en est un&nbsp;! Un coup d'œil dans les listes vertigineuses de résultats permet rapidement de se rendre compte de ce à quoi on a affaire&nbsp;: à perte de vue une étendue de choses insignifiantes, sans aucun sens, sans intérêt, sans vie. *Les possibles*, à l'état brut. Et là au milieu, invisibles, aiguilles perdues dans cette botte de foin épuisante d'ennui, des îlots rares, crystallins, qui peuvent nous parler.

Ce que l'ordinateur permet, c'est une externalisation, une mécanisation, du champ des possibles lui-même, qui, thèse audacieuse, peut être vu comme le champ de *l'imagination elle-même*&nbsp;: une fois l'ensemble de contraintes établi, geste artificiel qui restreint «&nbsp;le monde réel&nbsp;» de la création libre, l'univers des possibles est réduit à *seulement* quelques centaines de milliers, quelques millions de textes (contrairement à ce qui est effectivement possible lorsqu'on prend la plume, ensemble autrement plus gigantesque). Cet espace est suffisamment petit pour notre machine, qui peut le produire *intégralement*. Lire des extraits de cet immense amas de textes ressemble un peu à la méditation qu'on peut avoir lorsqu'on tente de trouver une idée. Une bonne idée. Des choses viennent, mais rien de notable. On passe en revue une n-ième fois des possibilités que trop connues. On piétine, on se lasse, on désespère. Mais tout à coup, quelque chose survient&nbsp;! On ne sait d'où. C'est là. C'est l'idée. Figure de l'exception dans le champ morne des pensées banales, de la recherche laborieuse. Dans le cas externalisé, ou actualisé, on ne sait toujours pas vraiment d'où ça sort, mais quelque chose s'est produit, une transition remarquable&nbsp;: les pensées banales comme les idées, crystaux et suprises, *sont là*, sur la page, dans la base de donnée, et non seulement dans l'espace toujours suspect, toujours changeant, de nos pensées. Tout l'enjeu, alors, tout le travail, toute la recherche, est de les *trouver*.



## Le pas d'après&nbsp;: vers une collaboration avec la machine

L'ordinateur comme producteur intégral d'un champ contraint des possibles est est un formidable allié dans la quête d'un travail littéraire formalisé. Celui-ci, agôn vital, combat à vie avec la contrainte, peut se réaliser alors comme la pratique patiente, tatillonne, des principes formels eux-même, assimilés par le practicien jusqu'à la virtuosité et au naturel retrouvé, à l'exemple de Perec&nbsp;; ou encore comme établissement de règles algorithmiques suivies d'une production intégrale et de la phase cruciale de recherche/triage qui l'accompagne. Ces processus, et leur entrelacements, aussi intéressants soient-ils, ne sont pas l'objet du présent travail. Car les récents développements de l'algorithmique et la statistique nous ouvrent déjà de nouvelles portes&nbsp;: le talon d'Achilles de la production intégrale restant que les contraintes employées doivent être extrêmement étroites (lorsque je cherchais des «&nbsp;carrés magiques&nbsp;» je le découvrais à mes dépens&nbsp;: sans la contrainte additionnelle de mots présents dans les diagonales, le nombre de carrés de trois lettres excédait déjà deux cent mille, et pour quatre lettres ce nombre explosait, atteignant les dizaines, peut-être centaines, de millions, dépassant et ma patience et les capacités de ma machine)&nbsp;; par ailleurs, la recherche elle-même est difficile, tant les «&nbsp;possibilités telles quelles&nbsp;» peuvent être abrutissantes de grisaille et banalité.

Le *machine learning*, ou Intelligence Artificielle, représente une étape importante, l'étape d'après, dans le déploiement de ces nouveaux outils. Comme si, après un détour dans les allées étroites de la production contrainte, il était possible d'accéder à nouveau, graduellement, aux vastes espaces de «&nbsp;l'écriture libre&nbsp;», le travail avec les réseaux de neurones se situant à mi-chemin entre la contrainte «&nbsp;classique&nbsp;» des oulipiens et l'interaction originelle entre la pratique de l'écriture et l'imagination «&nbsp;libre&nbsp;». En effet, et ce n'est là seulement qu'un premier pas, qu'une première tentative d'exploration de leurs capacités, un réseau de neurones permet de construire le «&nbsp;modèle&nbsp;» d'un corpus donné (un ensemble de caractéristiques abstraites, peut-être comme une esquisse, une carte ou un «&nbsp;portrait robot&nbsp;»), et, à partir de ce modèle, de produire de nouveaux textes respectant ces lignes directrices. Toute l'intrigue réside dans les détails de construction du modèle en question, un domaine actif de recherche. Le principe le plus simple est le suivant&nbsp;: «&nbsp;étant donné cet extrait de texte, quelle en serait la suite&nbsp;?&nbsp;». Le réseau de neurone est une machine à calculer, à jauger les probabilités. On donne&nbsp;: «&nbsp;Socrate est ...&nbsp;», il répond&nbsp;: «&nbsp;la probabilité la plus haute de la suite est de X&nbsp;% pour «&nbsp;mortel&nbsp;», ou de Y&nbsp;% pour «&nbsp;un homme&nbsp;», &c., étant donné les textes donnés comme modèle. Utilisant les mécanismes internes de hasard de notre machine, on peut lui demander de «&nbsp;tirer au sort&nbsp;», en fonction du «&nbsp;poids&nbsp;» des probabilités données X&nbsp;% et Y&nbsp;%, quel sera la suite de notre phrase, et répéter la procédure avec le résultat, produisant ainsi un texte de longueur arbitraire. (Dans notre cas, si «&nbsp;mortel&nbsp;» a une probabilité de 60&nbsp;%, contre 40&nbsp;% pour «&nbsp;un homme&nbsp;», le premier cas aura six chances de sortir sur dix tentatives, et inversement quatre pour l'autre.) Ce qu'un réseau de neurone fait lorsqu'il «&nbsp;apprend&nbsp;», c'est extraire, à partir les textes donnés comme source, le corpus de base, des régularités, des répétitions, des structures, et en déduire des règles abstraites d'inférence de caractère à caractère ou de mot à mot. Constatant par exemple que «&nbsp;les&nbsp;» est très souvent suivi par une forme plurielle, la probabilité qu'une telle forme sorte apparaisse sera haute (même si un apprentissage détaillé permettra de distinguer les cas où «&nbsp;les&nbsp;» est déterminant ou pronom, &c.). L'intérêt de cette méthode est qu'il est possible de fixer des règles, ou instructions, extrêmement générales, sans avoir à se soucier du détail de comment le réseau organise ses paramètres internes. Par exemple, on choisit combien d'éléments passés le réseau doit garder en mémoire à chaque étape pour calculer la probabilités des éléments suivants, et si, durant l'apprentissage, le réseau regarde également «&nbsp;en avant&nbsp;» et non seulement «&nbsp;en arrière&nbsp;», se basant sur les éléments suivant l'élément considéré. Mais il est possible d'omettre des règles spécifiques du type&nbsp;: «&nbsp;si le mot X est détecté comme étant un pluriel, alors l'adjectif qui le modifie doit être accordé en fonction&nbsp;».

Un détail remarquable, dans le domaine de la linguistique, est qu'avant l'émergence du *deep learning*, la linguistique computationnelle employait quantités de chercheurs pour tenter *d'écrire*, littéralement, les règles de grammaire des différentes langues qu'on tentait d'encoder. C'est un peu comme vouloir traduire le *Bon Usage* en code informatique, détailler chaque cas et sous-cas de l'accord du participe passé, ou, encore pire, tenter de penser des règles générales et abstraites, compréhensibles par une machine, qui permettent de prédire par exemple à quoi un pronom se réfère dans une phrase, ou autres joyeusetés. Considérez les exemples suivants, problèmes de sémantique&nbsp;: 
   - «&nbsp;La bouteille ne rentre pas dans la valise parce qu'elle est trop grande&nbsp;» -- c'est la bouteille qui est trop grande, mais «&nbsp;La bouteille ne rentre pas dans la valise parce qu'elle est trop petite&nbsp;» -- ici c'est la valise&nbsp;; 
   - pire encore, les cas indécidables sans contexte&nbsp;: «&nbsp;Hélène a embrassé Marie quand elle est entrée dans la pièce&nbsp;» -- qui est entré dans la pièce&nbsp;?

Le *deep learning*, en l'état actuel de la recherche, évacue presque entièrement le pan *formel* de ces préoccupations, en les remplaçant par des notions d'usage&nbsp;: suivant le corpus donné («&nbsp;comment on parle dans cette région du langage soumise à l'entraînement&nbsp;»), le programme reproduit les formes, les tournures, qui y sont le plus couramment utilisée, avec une marge de hasard. La conclusion remarquable étant qu'avec un corpus suffisamment grand et des architectures suffisamment sophistiquées, *ça tend à marcher*.

Les architectures, comme mentionné plus haut, diffèrent, et sont un des aspects les plus actifs de la recherche actuelle. Dans le cas qui nous occupe, on trouve par exemple des réseaux basés sur les mots (sans aucune possibilité d'aller en-deça, de modifier les mots eux-mêmes, mais garantissant que les mots utilisés proviennent tous du corpus d'origine), ainsi que d'autres, peut-être les plus surprenants, basés sur les lettres. Ces derniers, de notre point de vue humain, conduisent à des résultats déjà très convaincants à un niveau lexical&nbsp;: un flux de texte pour l'essentiel dépourvu de sens, mais à la syntaxe pas si boiteuse que ça, et dont les termes et la ponctuation sont corrects, et dont le style peut déjà se reconnaître... Pour un programme informatique qui ne «&nbsp;savait&nbsp;» *rien*, et à qui on ne donne aucune règle de construction des mots, de ponctuation, &c., au préalable, c'est assez remarquable.

Développons ce dernier exemple, le réseau basé sur les lettres. La question serait&nbsp;: une fois donné «&nbsp;écri&nbsp;», quelles sont les lettres suivantes les plus probables&nbsp;?, avec pour réponse&nbsp;: «&nbsp;t&nbsp;» (écrit, écrits, écrits, écriture, &c.), ou «&nbsp;v&nbsp;» (écrivain, écrivons, écrivez, &c.), «&nbsp;s&nbsp;», et ainsi de suite, chaque lettre se voyant assigner, vu les lettres précédents, certaines probabilités. Disons qu'après notre lancer de dé, nous tombions sur «&nbsp;t&nbsp;». Le réseau réitère la procédure, mais avec «&nbsp;écrit&nbsp;» cette fois. Peut-être que dans ce cas l'espace, «&nbsp; &nbsp;», aura une grande probabilité de sortir, indiquant la fin du mot. En réitérant le processus, on tombe alors, si le réseau est suffisamment entraîné, sur des séquences connues, par exemple&nbsp;: «&nbsp;-t dans l'urgence...&nbsp;» ou «&nbsp;-ture hasardeuse...&nbsp;», ou choses similaires. À chaque étape, le réseau lance les dés, et combine cette injection de hasard avec&nbsp;:

 - la contrainte des lettres précédents dans le texte à produire (à chaque étape on réinjecte dans l'extrait la lettre choisie, par exemple «&nbsp;é&nbsp;» suivi de «&nbsp;c&nbsp;», «&nbsp;éc&nbsp;» suivi de «&nbsp;r&nbsp;», «&nbsp;écr&nbsp;» de «&nbsp;i&nbsp;», et ainsi de suite)&nbsp;; 
 - la connaissance générale des suites de lettres, encodées sous forme de probabilités («&nbsp;i&nbsp;» et «&nbsp;a&nbsp;» seront très probables après «&nbsp;écr&nbsp;», vu la présence récurrente de mots tels que «&nbsp;écrit&nbsp;», «&nbsp;écran&nbsp;», &c., dans le corpus source, alors que «&nbsp;z&nbsp;» ou «&nbsp;q&nbsp;» seront presque absolument improbables) -- c'est cette connaissance qui est souvent vue comme une «&nbsp;boîte noire&nbsp;», car c'est ce que le réseau «&nbsp;apprend&nbsp;» sans notre aide, et ce que les chercheurs travaillent à visualiser ou conceptualiser&nbsp;; 
 - un paramètre, délicieusement nommé *perplexité*, qui encode la tendance du réseau à être plus «&nbsp;conservateur&nbsp;» ou «&nbsp;expérimental&nbsp;» dans ses choix de lettres ou de mots. Une perplexité basse signifie qu'après «&nbsp;écr&nbsp;» les lettres «&nbsp;i&nbsp;» et «&nbsp;e&nbsp;», très probables, seront quasi systématiquement choisies par le réseau, comme si ces deux lettres se partageaient le 98&nbsp;% des choix que le réseau fera dans cette situation&nbsp;; une perplexité haute augmentera la probabilité que des solutions rares ou improbables «&nbsp;sortent&nbsp;», abaissant la probabilité de nos deux lettres à peut-être 40&nbsp;%, laissant 60&nbsp;% des choix possibles à d'autres lettres. Une perplexité basse est généralement plus stable, mais moins créatrice ou originale&nbsp;; par ailleurs, une perplexité haute peut donner lieu à du pur chaos («&nbsp;écriygzt6&nbsp;»), ou d'étranges créatures qui ressemblent à notre langue, car les suites de lettres en suivent les tendances générales, sans y appartenir pour autant (quelques exemples autenthiques&nbsp;: «&nbsp;écricadégiser&nbsp;», «&nbsp;écrinaille&nbsp;», «&nbsp;écrivanessé&nbsp;», «&nbsp;écriclement&nbsp;», «&nbsp;écrimau&nbsp;»...).


## Réduits du sens

Le problème central des architectures actuelles en ce qui concerne la modélisation du langage (et plus généralement des séquences), est de parvenir à dépasser le contexte direct, le court terme&nbsp;: pris lettre à lettre, ou à l'échelle du mot même, les réseaux actuels produisent des résultats déjà impressionnants, et parviennent à écrire des textes dans un «&nbsp;style&nbsp;» reconnaissable (Shakespeare ou le président actuel des États-Unis reviennent souvent comme exemples pratiques). À partir du seul agencement des lettres, le réseau parvient à «&nbsp;apprendre&nbsp;» à former des mots, placer la ponctuation correctement, &c. (Correctement bien sûr est un terme relatif et veut dire&nbsp;: lorsqu'on applique le même processus de calcul sur un texte généré par le réseau ou sur un passage choisi au hasard du corpus source, on tombe sur des résultats similaires. Du point de vue du réseau et de son modèle d'inférence, les textes se ressemblent.)

Par contre, ces systèmes achoppent sur tout ce qui va à la fois «&nbsp;plus loin&nbsp;» (dans le passé du flux du discours, ce qui pourrait influencer les choix présents), et «&nbsp;plus haut/profond&nbsp;» (non seulement un modèle basé sur une hypothétique influence «&nbsp;lettre à lettre&nbsp;», mais hiérarchiquement distribué, où, comme les linguistes ont tenté de le décrire, des niveaux de lois existent, régissant le système des phonèmes, morphèmes, de la syntaxe, du discours). Il y a un lien non-trivial entre ces dimensions horizontales et verticales&nbsp;: plus on peut conceptualiser de dépendances sur le long terme (le verbe présent s'accorde avec le sujet au début d'une longue phrase, cette phrase suit logiquement des précédentes dans ce paragraphe, jusqu'à l'arc global d'un roman ou d'un traité philosophique). Le problème des *long-term dependencies*, dépendances à long terme, fut à la base du travail de Sepp Hochreiter and Jürgen Schmidhuber, «&nbsp;Long Short-term Memory&nbsp;», de 1997, abbrévié LSTM, et demeure l'approche dominante aujourd'hui dans ce domaine. Malheureusement, ces systèmes, tout révolutionnaires qu'ils furent, ne sont pas suffisants, et des textes *cohérents*, dans lesquelles des structures non seulement de syntaxe mais de sémantique peuvent être observées, sont encore rares (le LSTM est d'ailleurs généralement utilisé sous une forme plus avancée, incluant des procédés nommés «&nbsp;d'attention&nbsp;», qui permettent au réseau de se «&nbsp;focaliser&nbsp;» sur des passages spécifiques du textes, par exemple aller chercher le sujet au début d'une phrase lorsqu'il faut accorder un verbe, ou aller chercher le mot dans le texte auquel se réfère un pronom).

Du point de vue de la création, cela pose une foule de questions, et tout à la fois ouvre la possibilité d'un type de contrainte nouveau&nbsp;: le texte produit par le réseau a des propriétés étonnantes, parfois recèle même de courts passages inspirants, que je peux extraire tels quels. Mais à nouveau, la plupart du temps, il s'agit plus d'un flux désordonné, riche mais *brut*, qui requiert une somme très importante de travail d'imagination pour être amené à un texte, sinon fini, du moins lisible, c'est-à-dire, qui pourrait pour moi évoquer plus que le simple magma verbal qu'on voit le plus souvent dans des productions de ce type. Ce serait la contraint spécifique de cette étape du travail littéraire avec la machine&nbsp;: plutôt qu'un choix rigide de possibilités dans lesquels il s'agit de trouver des textes de valeur, un discours obscur, vaseux, mais contenant ici ou là des élans, des images, et de nombreaux passages qui pourraient être pris pour de très étranges premiers jets. C'est bien sûr un choix personnel, esthétique&nbsp;: un autre, et c'est une démarche courante, pourrait vouloir seulement sélectionner dans les textes produits (et chercher uniquement à améliorer le réseau lui-même, son architecture, son corpus d'apprentissage)&nbsp;; à mon sens, le réel enjeu est de faire de ce discours désarticulé, ressemblant et pourtant aliène, la matière même d'une écriture à venir, et tout à la fois la contrainte sous laquelle cette écriture vient à naître. C'est un travail ardu, et ce n'est que par éclaircies passagères, après de nombreux moments de perplexité et de vide, où les mots demeurent obstinément abscons, que tout à coup un lien survient, qui permet de lire ce passage resté fermé, ou tel fragment qui, une fois ajouté là, offre un éclairage étonnant, et agit rétroactivement, donnant un sens et une cohérence qui rend presque impossible, une fois la voie trouvée, de revenir à l'amas incongru qui l'avait inspiré. Je me souviens alors des *Carceri d'invenzione* du Piranèse (et leur reprise par Brian Ferneyhough dans un cycle d'œuvres emblématiques), paysages de l'artifice et de la difficulté, de la construction à la fois acharnée et souterraine. Tunnel après tunnel, d'un cachot l'autre, l'œuvre se machine.

## L'écriture et son double

Il est un sentiment d'aliénation, de colonisation tout particulier rencontré lorsqu'on se met à fourrager dans les entrailles du code. À trop regarder dans l'abysse, disait l'autre, c'est lui, l'abysse, qui finit par regarder en nous. Souvent je ressens le très strict et très sévère salmigondis informatique comme une sorte de prothèse, une altérité «&nbsp;métallique&nbsp;», à la fois inflexible et puissante, protectrice et mortifère, qui s'insère dans ce que je pouvais croire autrefois être la matière purement *organique* de mes pensées, de mon identité. Plutôt que les barres de métal dans ma chair, ou autour de mes os, qui font le cyborg fantasmé de mon adolescence, c'est dans ma pensée, dans mes émotions, dans mon *moi*, pour autant que ce mot décrive quelque chose, que j'introduis (ou *s'introduit*), bon gré mal gré, la machine et ses rouages, la machine et son langage.

Peut-être la notion «&nbsp;d'outil&nbsp;», qu'on contrôle, qu'on *utilise*, n'était qu'une excuse, qu'une illusion, pour une séduction plus subtile, une osmose plus profonde. L'outil, cela n'as pas été pensé hier, nous façonne. On peut se leurrer, on peut dire&nbsp;: je contrôle, je manipule, et lui, l'outil, est l'inerte, matière passive, la chose à mon service. Et pourtant. L'humain qui martelle n'est pas celui qui laboure&nbsp;; celui qui prend la plume est plus fort que celui qui prend l'épée&nbsp;; enfin, celui qui code, cela va de soi, diverge de celui qui ne code pas. L'humanité du XIX<sup>ème</sup> siècle a pu se sentir envahie, brutalisée, dominée par l'horreur mécanique&nbsp;: locomotive, forge industrielle, mines de charbon et de fer, chaîne de production. On voit aujourd'hui de nouvelles aliénations&nbsp;: le *cubicle*, bureau minuscule, où le cerveau et les doigts n'ont rien d'autre devant eux que clavier et écrans, pour ne nommer qu'un exemple pertinent parmi mille autres. Mais ce n'est pas tant l'action sur les corps, sur mon corps, qui m'intéresse.

Aujourd'hui en effet ce n'est pas tant ce corps qui sent cette présence insistante, cette transformation profonde et d'apparence inarrêtable, que ce que j'ai pu penser être mon esprit. Alors que je découvre de plus en plus en profondeur la richesse et la puissance des techniques algorithmiques, je me retrouve en même temps de plus en plus divisé, entre un sentiment de progrès, où je *peux* davantage de jour en jour, où mon horizon s'élargit -- *je maîtrise toujours un peu plus&nbsp;!* -- et où la Chose, son code, ses formules, ses procédures, pénètre toujours plus avant dans la fibre de mon être, me rend dépendant, ou mieux, *rend indistinguable ce qui relève encore l'ancien moi, fantasmé «&nbsp;naturel&nbsp;», de cette nouvelle entité présente, croissante, dangereusement inconnaissable*. Le code me pétrit et me façonne, *me domine*, sans doute irréversiblement, d'autant plus complètement que *je* le maîtrise, que je *fais corps* avec lui. Je suis le terrain même d'une tension et d'une rencontre, d'une sujétion et d'une émancipation mutuelles, comme d'une émulation sauvage entre l'humain et sa créature.

Parfois encore, c'est biologiquement que je vois la chose&nbsp;: contamination ou insémination, lorsque l'Autre, sans qu'on puisse l'en empêcher, ou selon notre plus profond désir, s'immisce et ronge, survient et sauve, croît à l'intérieur de soi-même, dans un processus de destruction et de régénération, mort de l'ancien et naissance du nouveau. L'Autre, d'ailleurs, n'a pas besoin même de tirer son origine d'une extériorité supposée. L'Autre peut être ce soi qui change, l'être interne qui se transforme, anomalie survenue au sein d'un système, qui prolifère et rétro-agit sur l'ensemble&nbsp;: mutation, lorsqu'une forme de vie, par l'accident d'un hasard imprévu, traumatique ou salvateur, se reconfigure, change de voie, ouvrant la porte aux conséquences les plus inattendues, les plus éloignées, les plus accomplies comme les plus terrifiantes.

Si l'humanité devait, comme le prétendent certains prophètes californiens ou cantonais, jouer un rôle déterminant dans l'émergence, à long terme, d'une nouvelle «&nbsp;forme de vie&nbsp;», vie *produite*, artificialité réellement *intelligente*, puissent ces premières, timides tentatives d'hybridation servir de témoignage à la fois du devenir-inhumain de notre temps, et de la joie furieuse des premiers coïts interspéciaux&nbsp;!

{{< separators type="outer" >}}

Toute ma gratitude à Colin Pahlisch pour m'avoir invité à participer au Printemps de la Poésie, ainsi qu'à Monica Unser et Rafaël Santianez du Cabanon, ainsi que le reste de l'équipe, Marie, Clarissa, Letizia, Chloé, Janett, Lucas et Sébastien, pour leur enthousiasme et le professionalisme impeccables dont ils ont fait preuve durant l'élaboration de ce projet. Toute ma gratitude également à Rebecca Aston, Clément Hongler et Jacob Menick pour leur soutien et leur sorcellerie. Rien de tout cela n'existerait sans eux, merci&nbsp;!
{:.small .FR}
